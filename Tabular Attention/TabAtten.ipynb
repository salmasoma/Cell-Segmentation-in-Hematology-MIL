{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tab-transformer-pytorch\n",
      "  Using cached tab_transformer_pytorch-0.3.0-py3-none-any.whl.metadata (690 bytes)\n",
      "Requirement already satisfied: einops>=0.3 in /Users/salma/anaconda3/envs/tensor/lib/python3.10/site-packages (from tab-transformer-pytorch) (0.3.2)\n",
      "Requirement already satisfied: torch>=1.6 in /Users/salma/anaconda3/envs/tensor/lib/python3.10/site-packages (from tab-transformer-pytorch) (1.13.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/salma/anaconda3/envs/tensor/lib/python3.10/site-packages (from torch>=1.6->tab-transformer-pytorch) (4.8.0)\n",
      "Using cached tab_transformer_pytorch-0.3.0-py3-none-any.whl (6.9 kB)\n",
      "Installing collected packages: tab-transformer-pytorch\n",
      "Successfully installed tab-transformer-pytorch-0.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tab-transformer-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "# Load the dataset\n",
    "data_train = pd.read_csv('/Users/salma/Downloads/features_train.csv')\n",
    "data_test= pd.read_csv('/Users/salma/Downloads/features_test.csv')\n",
    "\n",
    "\n",
    "# Assume 'target' is the name of your target column\n",
    "target = 'Target'  # Replace this with your actual target column name\n",
    "\n",
    "features_train = data_train.columns.drop([target, \"Image\"])  # Assuming the rest are features\n",
    "features_test = data_test.columns.drop([target, \"Image\"])  # Assuming the rest are features\n",
    "\n",
    "# Normalize continuous columns\n",
    "scaler = StandardScaler()\n",
    "data_train[features_train] = scaler.fit_transform(data_train[features_train])\n",
    "data_test[features_test] = scaler.transform(data_test[features_test])\n",
    "\n",
    "X_train = data_train[features_train]\n",
    "X_test = data_test[features_test]\n",
    "\n",
    "#label encoding the target column\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "data_train[target] = le.fit_transform(data_train[target])\n",
    "data_test[target] = le.transform(data_test[target])\n",
    "\n",
    "y_train = data_train[target].values\n",
    "y_test = data_test[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tab_transformer_pytorch import TabTransformer, FTTransformer\n",
    "import torch\n",
    "from torch.nn import ReLU\n",
    "\n",
    "num_continuous = X_train.shape[1]\n",
    "dim_output = len(pd.unique(data_train[target]))\n",
    "\n",
    "model = TabTransformer(\n",
    "    categories=[],  # no categorical features\n",
    "    num_continuous=num_continuous,  # number of continuous columns\n",
    "    dim_out=dim_output,  # number of classes\n",
    "    depth=8,  # depth of the transformer\n",
    "    heads=12,  # attention heads\n",
    "    attn_dropout=0.4,\n",
    "    ff_dropout=0.4,\n",
    "    dim = 64,\n",
    "    mlp_hidden_mults=(128, 64),  # multiples of each hidden layer in the final MLP\n",
    "    mlp_act=ReLU()  # activation of the final MLP\n",
    ")\n",
    "\n",
    "model = FTTransformer(\n",
    "    categories = (),      # tuple containing the number of unique values within each category\n",
    "    num_continuous = num_continuous,                # number of continuous values\n",
    "    dim = 32,                           # dimension, paper set at 32\n",
    "    dim_out = 5,                        \n",
    "    depth = 6,                          # depth, paper recommended 6\n",
    "    heads = 8,                          # heads, paper recommends 8\n",
    "    attn_dropout = 0.1,                 # post-attention dropout\n",
    "    ff_dropout = 0.1                    # feed forward dropout\n",
    ")\n",
    "\n",
    "\n",
    "# Convert data to torch tensors\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Define your loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X_cat, X_test_tensor, y_test_tensor):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        outputs = model(X_cat, X_test_tensor)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total = y_test_tensor.size(0)\n",
    "        correct = (predicted == y_test_tensor).sum().item()\n",
    "        print(f'Accuracy: {100 * correct / total:.2f}%')\n",
    "        return 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.6020207405090332\n",
      "Accuracy: 25.83%\n",
      "Model saved!\n",
      "Epoch 2, Loss: 10.513237953186035\n",
      "Accuracy: 47.80%\n",
      "Model saved!\n",
      "Epoch 3, Loss: 12.197381019592285\n",
      "Accuracy: 50.73%\n",
      "Model saved!\n",
      "Epoch 4, Loss: 10.233697891235352\n",
      "Accuracy: 25.83%\n",
      "Epoch 5, Loss: 9.128161430358887\n",
      "Accuracy: 43.81%\n",
      "Epoch 6, Loss: 4.014889240264893\n",
      "Accuracy: 51.00%\n",
      "Model saved!\n",
      "Epoch 7, Loss: 2.8073935508728027\n",
      "Accuracy: 49.93%\n",
      "Epoch 8, Loss: 2.126082181930542\n",
      "Accuracy: 50.07%\n",
      "Epoch 9, Loss: 1.7405215501785278\n",
      "Accuracy: 47.80%\n",
      "Epoch 10, Loss: 1.4988607168197632\n",
      "Accuracy: 44.87%\n",
      "Epoch 11, Loss: 1.3496067523956299\n",
      "Accuracy: 43.81%\n",
      "Epoch 12, Loss: 1.2527309656143188\n",
      "Accuracy: 43.54%\n",
      "Epoch 13, Loss: 1.1439390182495117\n",
      "Accuracy: 44.07%\n",
      "Epoch 14, Loss: 1.046669840812683\n",
      "Accuracy: 46.21%\n",
      "Epoch 15, Loss: 0.9824427366256714\n",
      "Accuracy: 51.13%\n",
      "Model saved!\n",
      "Epoch 16, Loss: 0.9556560516357422\n",
      "Accuracy: 56.86%\n",
      "Model saved!\n",
      "Epoch 17, Loss: 0.957031786441803\n",
      "Accuracy: 60.85%\n",
      "Model saved!\n",
      "Epoch 18, Loss: 0.9662835001945496\n",
      "Accuracy: 64.98%\n",
      "Model saved!\n",
      "Epoch 19, Loss: 0.96586012840271\n",
      "Accuracy: 67.24%\n",
      "Model saved!\n",
      "Epoch 20, Loss: 0.9441261291503906\n",
      "Accuracy: 67.91%\n",
      "Model saved!\n",
      "Epoch 21, Loss: 0.9035524725914001\n",
      "Accuracy: 66.58%\n",
      "Epoch 22, Loss: 0.8571342825889587\n",
      "Accuracy: 63.91%\n",
      "Epoch 23, Loss: 0.8133037090301514\n",
      "Accuracy: 61.65%\n",
      "Epoch 24, Loss: 0.7715027332305908\n",
      "Accuracy: 59.79%\n",
      "Epoch 25, Loss: 0.734679102897644\n",
      "Accuracy: 59.65%\n",
      "Epoch 26, Loss: 0.7078840136528015\n",
      "Accuracy: 58.99%\n",
      "Epoch 27, Loss: 0.6890622973442078\n",
      "Accuracy: 58.19%\n",
      "Epoch 28, Loss: 0.669546902179718\n",
      "Accuracy: 57.79%\n",
      "Epoch 29, Loss: 0.6473472118377686\n",
      "Accuracy: 58.32%\n",
      "Epoch 30, Loss: 0.6285549998283386\n",
      "Accuracy: 58.46%\n",
      "Epoch 31, Loss: 0.6169987320899963\n",
      "Accuracy: 58.85%\n",
      "Epoch 32, Loss: 0.6070297956466675\n",
      "Accuracy: 59.39%\n",
      "Epoch 33, Loss: 0.5929456353187561\n",
      "Accuracy: 59.52%\n",
      "Epoch 34, Loss: 0.5774523615837097\n",
      "Accuracy: 59.39%\n",
      "Epoch 35, Loss: 0.5630437135696411\n",
      "Accuracy: 59.52%\n",
      "Epoch 36, Loss: 0.5493988394737244\n",
      "Accuracy: 58.19%\n",
      "Epoch 37, Loss: 0.537225604057312\n",
      "Accuracy: 57.92%\n",
      "Epoch 38, Loss: 0.5253019332885742\n",
      "Accuracy: 58.06%\n",
      "Epoch 39, Loss: 0.5117598176002502\n",
      "Accuracy: 58.32%\n",
      "Epoch 40, Loss: 0.4991293251514435\n",
      "Accuracy: 59.25%\n",
      "Epoch 41, Loss: 0.489829957485199\n",
      "Accuracy: 59.92%\n",
      "Epoch 42, Loss: 0.481344610452652\n",
      "Accuracy: 60.05%\n",
      "Epoch 43, Loss: 0.47130143642425537\n",
      "Accuracy: 60.05%\n",
      "Epoch 44, Loss: 0.4614620506763458\n",
      "Accuracy: 60.72%\n",
      "Epoch 45, Loss: 0.4524877071380615\n",
      "Accuracy: 60.45%\n",
      "Epoch 46, Loss: 0.44297903776168823\n",
      "Accuracy: 60.32%\n",
      "Epoch 47, Loss: 0.43348774313926697\n",
      "Accuracy: 60.32%\n",
      "Epoch 48, Loss: 0.42409420013427734\n",
      "Accuracy: 60.59%\n",
      "Epoch 49, Loss: 0.4149375855922699\n",
      "Accuracy: 60.19%\n",
      "Epoch 50, Loss: 0.40656301379203796\n",
      "Accuracy: 59.52%\n",
      "Epoch 51, Loss: 0.39793068170547485\n",
      "Accuracy: 59.79%\n",
      "Epoch 52, Loss: 0.38918665051460266\n",
      "Accuracy: 59.65%\n",
      "Epoch 53, Loss: 0.3806777000427246\n",
      "Accuracy: 59.65%\n",
      "Epoch 54, Loss: 0.37195324897766113\n",
      "Accuracy: 59.65%\n",
      "Epoch 55, Loss: 0.3636613190174103\n",
      "Accuracy: 60.45%\n",
      "Epoch 56, Loss: 0.355390727519989\n",
      "Accuracy: 60.85%\n",
      "Epoch 57, Loss: 0.34728336334228516\n",
      "Accuracy: 61.25%\n",
      "Epoch 58, Loss: 0.3392549455165863\n",
      "Accuracy: 61.25%\n",
      "Epoch 59, Loss: 0.3308984339237213\n",
      "Accuracy: 60.85%\n",
      "Epoch 60, Loss: 0.32289260625839233\n",
      "Accuracy: 60.19%\n",
      "Epoch 61, Loss: 0.3149934411048889\n",
      "Accuracy: 60.19%\n",
      "Epoch 62, Loss: 0.3069104552268982\n",
      "Accuracy: 60.45%\n",
      "Epoch 63, Loss: 0.29901421070098877\n",
      "Accuracy: 60.45%\n",
      "Epoch 64, Loss: 0.2914847731590271\n",
      "Accuracy: 60.72%\n",
      "Epoch 65, Loss: 0.28373613953590393\n",
      "Accuracy: 60.19%\n",
      "Epoch 66, Loss: 0.27603545784950256\n",
      "Accuracy: 59.65%\n",
      "Epoch 67, Loss: 0.2683872580528259\n",
      "Accuracy: 59.52%\n",
      "Epoch 68, Loss: 0.2610268294811249\n",
      "Accuracy: 59.65%\n",
      "Epoch 69, Loss: 0.25358080863952637\n",
      "Accuracy: 59.39%\n",
      "Epoch 70, Loss: 0.2461959421634674\n",
      "Accuracy: 59.79%\n",
      "Epoch 71, Loss: 0.2390362024307251\n",
      "Accuracy: 59.65%\n",
      "Epoch 72, Loss: 0.2319052666425705\n",
      "Accuracy: 59.65%\n",
      "Epoch 73, Loss: 0.2248617261648178\n",
      "Accuracy: 59.25%\n",
      "Epoch 74, Loss: 0.21797116100788116\n",
      "Accuracy: 59.12%\n",
      "Epoch 75, Loss: 0.21113334596157074\n",
      "Accuracy: 59.39%\n",
      "Epoch 76, Loss: 0.20445393025875092\n",
      "Accuracy: 59.39%\n",
      "Epoch 77, Loss: 0.19785912334918976\n",
      "Accuracy: 59.25%\n",
      "Epoch 78, Loss: 0.19148163497447968\n",
      "Accuracy: 59.25%\n",
      "Epoch 79, Loss: 0.18507087230682373\n",
      "Accuracy: 59.25%\n",
      "Epoch 80, Loss: 0.17881277203559875\n",
      "Accuracy: 59.12%\n",
      "Epoch 81, Loss: 0.17265872657299042\n",
      "Accuracy: 59.12%\n",
      "Epoch 82, Loss: 0.16659046709537506\n",
      "Accuracy: 59.12%\n",
      "Epoch 83, Loss: 0.16069573163986206\n",
      "Accuracy: 59.39%\n",
      "Epoch 84, Loss: 0.1549372673034668\n",
      "Accuracy: 59.39%\n",
      "Epoch 85, Loss: 0.14930586516857147\n",
      "Accuracy: 59.25%\n",
      "Epoch 86, Loss: 0.14382407069206238\n",
      "Accuracy: 59.39%\n",
      "Epoch 87, Loss: 0.13844524323940277\n",
      "Accuracy: 59.52%\n",
      "Epoch 88, Loss: 0.1332036554813385\n",
      "Accuracy: 59.65%\n",
      "Epoch 89, Loss: 0.12813688814640045\n",
      "Accuracy: 59.79%\n",
      "Epoch 90, Loss: 0.12319928407669067\n",
      "Accuracy: 59.65%\n",
      "Epoch 91, Loss: 0.11839454621076584\n",
      "Accuracy: 59.65%\n",
      "Epoch 92, Loss: 0.113740935921669\n",
      "Accuracy: 59.52%\n",
      "Epoch 93, Loss: 0.10920415818691254\n",
      "Accuracy: 59.52%\n",
      "Epoch 94, Loss: 0.10478586703538895\n",
      "Accuracy: 59.65%\n",
      "Epoch 95, Loss: 0.10050059109926224\n",
      "Accuracy: 59.25%\n",
      "Epoch 96, Loss: 0.09640669077634811\n",
      "Accuracy: 59.65%\n",
      "Epoch 97, Loss: 0.09240887314081192\n",
      "Accuracy: 59.39%\n",
      "Epoch 98, Loss: 0.0885302871465683\n",
      "Accuracy: 59.39%\n",
      "Epoch 99, Loss: 0.08478931337594986\n",
      "Accuracy: 59.25%\n",
      "Epoch 100, Loss: 0.08121654391288757\n",
      "Accuracy: 59.25%\n",
      "Epoch 101, Loss: 0.07775821536779404\n",
      "Accuracy: 59.12%\n",
      "Epoch 102, Loss: 0.07447290420532227\n",
      "Accuracy: 58.99%\n",
      "Epoch 103, Loss: 0.07129545509815216\n",
      "Accuracy: 59.25%\n",
      "Epoch 104, Loss: 0.06826180964708328\n",
      "Accuracy: 58.85%\n",
      "Epoch 105, Loss: 0.06538110971450806\n",
      "Accuracy: 58.85%\n",
      "Epoch 106, Loss: 0.06256880611181259\n",
      "Accuracy: 59.12%\n",
      "Epoch 107, Loss: 0.05987340956926346\n",
      "Accuracy: 59.12%\n",
      "Epoch 108, Loss: 0.05729169026017189\n",
      "Accuracy: 58.99%\n",
      "Epoch 109, Loss: 0.05480324849486351\n",
      "Accuracy: 58.99%\n",
      "Epoch 110, Loss: 0.05246087536215782\n",
      "Accuracy: 59.12%\n",
      "Epoch 111, Loss: 0.05022026225924492\n",
      "Accuracy: 59.12%\n",
      "Epoch 112, Loss: 0.048061393201351166\n",
      "Accuracy: 59.12%\n",
      "Epoch 113, Loss: 0.046025291085243225\n",
      "Accuracy: 58.85%\n",
      "Epoch 114, Loss: 0.0440823957324028\n",
      "Accuracy: 59.39%\n",
      "Epoch 115, Loss: 0.04220110923051834\n",
      "Accuracy: 59.12%\n",
      "Epoch 116, Loss: 0.04043857753276825\n",
      "Accuracy: 59.12%\n",
      "Epoch 117, Loss: 0.03872813656926155\n",
      "Accuracy: 59.12%\n",
      "Epoch 118, Loss: 0.03712356463074684\n",
      "Accuracy: 59.25%\n",
      "Epoch 119, Loss: 0.035598039627075195\n",
      "Accuracy: 59.25%\n",
      "Epoch 120, Loss: 0.03413306549191475\n",
      "Accuracy: 59.25%\n",
      "Epoch 121, Loss: 0.03276320919394493\n",
      "Accuracy: 59.25%\n",
      "Epoch 122, Loss: 0.031475767493247986\n",
      "Accuracy: 59.25%\n",
      "Epoch 123, Loss: 0.030193684622645378\n",
      "Accuracy: 59.25%\n",
      "Epoch 124, Loss: 0.029014917090535164\n",
      "Accuracy: 59.12%\n",
      "Epoch 125, Loss: 0.0278835017234087\n",
      "Accuracy: 59.12%\n",
      "Epoch 126, Loss: 0.026802603155374527\n",
      "Accuracy: 59.12%\n",
      "Epoch 127, Loss: 0.025783542543649673\n",
      "Accuracy: 59.25%\n",
      "Epoch 128, Loss: 0.024823490530252457\n",
      "Accuracy: 59.12%\n",
      "Epoch 129, Loss: 0.023886550217866898\n",
      "Accuracy: 59.25%\n",
      "Epoch 130, Loss: 0.022998154163360596\n",
      "Accuracy: 59.25%\n",
      "Epoch 131, Loss: 0.022175895050168037\n",
      "Accuracy: 59.25%\n",
      "Epoch 132, Loss: 0.021380715072155\n",
      "Accuracy: 59.25%\n",
      "Epoch 133, Loss: 0.020631441846489906\n",
      "Accuracy: 59.25%\n",
      "Epoch 134, Loss: 0.01992354355752468\n",
      "Accuracy: 59.25%\n",
      "Epoch 135, Loss: 0.019234253093600273\n",
      "Accuracy: 59.25%\n",
      "Epoch 136, Loss: 0.01858186163008213\n",
      "Accuracy: 59.25%\n",
      "Epoch 137, Loss: 0.017963306978344917\n",
      "Accuracy: 59.25%\n",
      "Epoch 138, Loss: 0.017383532598614693\n",
      "Accuracy: 59.25%\n",
      "Epoch 139, Loss: 0.01680983603000641\n",
      "Accuracy: 59.39%\n",
      "Epoch 140, Loss: 0.016276013106107712\n",
      "Accuracy: 59.25%\n",
      "Epoch 141, Loss: 0.015762420371174812\n",
      "Accuracy: 59.25%\n",
      "Epoch 142, Loss: 0.015278622508049011\n",
      "Accuracy: 59.12%\n",
      "Epoch 143, Loss: 0.014820722863078117\n",
      "Accuracy: 59.12%\n",
      "Epoch 144, Loss: 0.01436918880790472\n",
      "Accuracy: 58.99%\n",
      "Epoch 145, Loss: 0.01394682377576828\n",
      "Accuracy: 59.12%\n",
      "Epoch 146, Loss: 0.013543122448027134\n",
      "Accuracy: 58.99%\n",
      "Epoch 147, Loss: 0.013156230561435223\n",
      "Accuracy: 58.99%\n",
      "Epoch 148, Loss: 0.01278520468622446\n",
      "Accuracy: 58.85%\n",
      "Epoch 149, Loss: 0.012427132576704025\n",
      "Accuracy: 58.99%\n",
      "Epoch 150, Loss: 0.012088253162801266\n",
      "Accuracy: 58.99%\n",
      "Epoch 151, Loss: 0.01175831351429224\n",
      "Accuracy: 58.99%\n",
      "Epoch 152, Loss: 0.011443343944847584\n",
      "Accuracy: 58.99%\n",
      "Epoch 153, Loss: 0.011138713918626308\n",
      "Accuracy: 58.85%\n",
      "Epoch 154, Loss: 0.010853310115635395\n",
      "Accuracy: 58.85%\n",
      "Epoch 155, Loss: 0.010573862120509148\n",
      "Accuracy: 58.85%\n",
      "Epoch 156, Loss: 0.010311441496014595\n",
      "Accuracy: 58.85%\n",
      "Epoch 157, Loss: 0.010050675831735134\n",
      "Accuracy: 58.85%\n",
      "Epoch 158, Loss: 0.009807069785892963\n",
      "Accuracy: 58.85%\n",
      "Epoch 159, Loss: 0.009573368355631828\n",
      "Accuracy: 58.99%\n",
      "Epoch 160, Loss: 0.00933942012488842\n",
      "Accuracy: 58.99%\n",
      "Epoch 161, Loss: 0.009130341932177544\n",
      "Accuracy: 58.99%\n",
      "Epoch 162, Loss: 0.008917382918298244\n",
      "Accuracy: 58.99%\n",
      "Epoch 163, Loss: 0.008719553239643574\n",
      "Accuracy: 58.99%\n",
      "Epoch 164, Loss: 0.008516744710505009\n",
      "Accuracy: 58.99%\n",
      "Epoch 165, Loss: 0.008333475328981876\n",
      "Accuracy: 58.99%\n",
      "Epoch 166, Loss: 0.008149060420691967\n",
      "Accuracy: 58.85%\n",
      "Epoch 167, Loss: 0.00797267910093069\n",
      "Accuracy: 58.85%\n",
      "Epoch 168, Loss: 0.0077981241047382355\n",
      "Accuracy: 58.99%\n",
      "Epoch 169, Loss: 0.007633409462869167\n",
      "Accuracy: 58.99%\n",
      "Epoch 170, Loss: 0.007477829698473215\n",
      "Accuracy: 58.99%\n",
      "Epoch 171, Loss: 0.0073222508653998375\n",
      "Accuracy: 58.85%\n",
      "Epoch 172, Loss: 0.007173460442572832\n",
      "Accuracy: 58.85%\n",
      "Epoch 173, Loss: 0.007029401604086161\n",
      "Accuracy: 58.85%\n",
      "Epoch 174, Loss: 0.006890090648084879\n",
      "Accuracy: 58.85%\n",
      "Epoch 175, Loss: 0.00675324909389019\n",
      "Accuracy: 58.85%\n",
      "Epoch 176, Loss: 0.0066261300817132\n",
      "Accuracy: 58.85%\n",
      "Epoch 177, Loss: 0.0064968690276145935\n",
      "Accuracy: 58.85%\n",
      "Epoch 178, Loss: 0.00637626089155674\n",
      "Accuracy: 58.85%\n",
      "Epoch 179, Loss: 0.006254727486521006\n",
      "Accuracy: 58.85%\n",
      "Epoch 180, Loss: 0.006141728721559048\n",
      "Accuracy: 58.85%\n",
      "Epoch 181, Loss: 0.0060292985290288925\n",
      "Accuracy: 58.85%\n",
      "Epoch 182, Loss: 0.005920831114053726\n",
      "Accuracy: 58.85%\n",
      "Epoch 183, Loss: 0.005815279204398394\n",
      "Accuracy: 58.85%\n",
      "Epoch 184, Loss: 0.005713972728699446\n",
      "Accuracy: 58.85%\n",
      "Epoch 185, Loss: 0.005615489557385445\n",
      "Accuracy: 58.85%\n",
      "Epoch 186, Loss: 0.005515382159501314\n",
      "Accuracy: 58.85%\n",
      "Epoch 187, Loss: 0.0054243383929133415\n",
      "Accuracy: 58.85%\n",
      "Epoch 188, Loss: 0.0053323982283473015\n",
      "Accuracy: 58.85%\n",
      "Epoch 189, Loss: 0.005242366809397936\n",
      "Accuracy: 58.85%\n",
      "Epoch 190, Loss: 0.00515590887516737\n",
      "Accuracy: 58.85%\n",
      "Epoch 191, Loss: 0.00507184537127614\n",
      "Accuracy: 58.85%\n",
      "Epoch 192, Loss: 0.004989450331777334\n",
      "Accuracy: 58.85%\n",
      "Epoch 193, Loss: 0.004909219220280647\n",
      "Accuracy: 58.85%\n",
      "Epoch 194, Loss: 0.004830920137465\n",
      "Accuracy: 58.85%\n",
      "Epoch 195, Loss: 0.004755465313792229\n",
      "Accuracy: 58.85%\n",
      "Epoch 196, Loss: 0.004682904575020075\n",
      "Accuracy: 58.85%\n",
      "Epoch 197, Loss: 0.004609334282577038\n",
      "Accuracy: 58.85%\n",
      "Epoch 198, Loss: 0.0045391106978058815\n",
      "Accuracy: 58.85%\n",
      "Epoch 199, Loss: 0.004469844978302717\n",
      "Accuracy: 58.85%\n",
      "Epoch 200, Loss: 0.004405446350574493\n",
      "Accuracy: 58.85%\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "#empty categorical array\n",
    "X_cat = np.array([])\n",
    "best_accuracy = 0\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_cat, X_train_tensor)\n",
    "    loss = criterion(output, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
    "    test_Accuracy = evaluate(model, X_cat, X_test_tensor, y_test_tensor)\n",
    "    if test_Accuracy > best_accuracy:\n",
    "        best_accuracy = test_Accuracy\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print('Model saved!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the best model\n",
    "model.load_state_dict(torch.load('best_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 67.91%\n",
      "F1 Score: 68.36\n",
      "Precision: 69.08\n",
      "Recall: 67.91\n",
      "Confusion Matrix:\n",
      "[[  2   1  11   1   5]\n",
      " [ 16  58  38  13  14]\n",
      " [  6  60 120   2   3]\n",
      " [  1  11   5 132   2]\n",
      " [  1   9  27  15 198]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "#calculate f1 score, precision, recall\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    outputs = model(X_cat, X_test_tensor)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total = y_test_tensor.size(0)\n",
    "    correct = (predicted == y_test_tensor).sum().item()\n",
    "    print(f'Accuracy: {100 * correct / total:.2f}%')\n",
    "    f1 = f1_score(y_test_tensor, predicted, average='weighted')\n",
    "    precision = precision_score(y_test_tensor, predicted, average='weighted')\n",
    "    recall = recall_score(y_test_tensor, predicted, average='weighted')\n",
    "    print(f'F1 Score: {100*f1:.2f}')\n",
    "    print(f'Precision: {100*precision:.2f}')\n",
    "    print(f'Recall: {100*recall:.2f}')\n",
    "    cm = confusion_matrix(y_test_tensor, predicted)\n",
    "    print('Confusion Matrix:')\n",
    "    print(cm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
